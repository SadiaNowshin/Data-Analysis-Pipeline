{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cTfaOghgLbx"
   },
   "source": [
    "# Assignment 3: Building a Data Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4xGocolW45t"
   },
   "source": [
    "The SQL code for the analysis problems\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhzBzMHygWam"
   },
   "source": [
    "### Part 1. Basic analytics using BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cvkxPM6MKtK"
   },
   "outputs": [],
   "source": [
    " ## P1. Count the number of rows in the all_sessions_raw table (0 point)\n",
    " SELECT COUNT(*) FROM `data-to-insights.ecommerce.all_sessions_raw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYCTg9ZxgkJ4"
   },
   "outputs": [],
   "source": [
    "## P2. Count the number of rows in the all_sessions table (0 point)\n",
    "SELECT COUNT(*) FROM `data-to-insights.ecommerce.all_sessions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebTnRR9ogrJW"
   },
   "outputs": [],
   "source": [
    "## P3. Check the min and max date of the records in the all_sessions (use the date column) (0 points).\n",
    "SELECT MIN(date), MAX(date) FROM `data-to-insights.ecommerce.all_sessions` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0XK7mtPgxSz"
   },
   "outputs": [],
   "source": [
    "## P4. Write a query that shows total unique visitors. (Your query determines the total views by counting product_views and the number of unique visitors by counting fullVisitorID) (5 points)\n",
    "select\n",
    "  COUNT(DISTINCT fullVisitorID) + COUNT(DISTINCT pageviews) as total_unique_visitor\n",
    "from `data-to-insights.ecommerce.all_sessions`  where pageviews is not null; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcSHqh0Xg8Jd"
   },
   "outputs": [],
   "source": [
    "## P5. Write a query that shows total unique visitors (fullVisitorID) by the referring site (channelGrouping) (5 points).\n",
    "SELECT channelGrouping, COUNT(distinct fullVisitorID) as Unique_Visitors FROM `data-to-insights.ecommerce.all_sessions` group by channelGrouping ORDER BY Unique_Visitors;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hVoy6H-hEgW"
   },
   "outputs": [],
   "source": [
    "## P6. Write a query to list all the unique product names (v2ProductName) alphabetically (5 points).\n",
    "SELECT DISTINCT(v2ProductName) FROM `data-to-insights.ecommerce.all_sessions`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkEY0k-PhOa0"
   },
   "source": [
    "### Part 2. Duplicate detection in data (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZFFMDBSgrHs"
   },
   "outputs": [],
   "source": [
    "## P7. Write a query that returns the number of duplicate records in the “all_sessions_raw” table. (Hint: you need to group the records by all the columns in the table) (5 points).\n",
    "SELECT *,COUNT(*)\n",
    "FROM `data-to-insights.ecommerce.all_sessions_raw`\n",
    "GROUP BY fullVisitorId,channelGrouping,time,country,city, totaltransactionRevenue,transactions,timeOnSite, pageviews,sessionQualityDim,date,visitId,type,productRefundAmount,productQuantity,productPrice,productRevenue,productSKU,v2ProductName,v2ProductCategory,productVariant,currencyCode,itemQuantity,itemRevenue,transactionRevenue,transactionId,pageTitle,searchKeyword,pagePathLevel1,eCommerceAction_type,eCommerceAction_step,eCommerceAction_option\n",
    "HAVING COUNT(*) > 1 ORDER BY COUNT(*)deSC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkUjBmg1heGe"
   },
   "outputs": [],
   "source": [
    "## P8. Write a query that ensures there are no duplicate records in the “all_sessions” table (5 points).\n",
    "SELECT *, Count(*) FROM `data-to-insights.ecommerce.all_sessions` \n",
    "group by fullVisitorId, channelGrouping, time, country, city, totalTransactionRevenue, transactions, timeOnSite, pageviews, sessionQualityDim, date,  visitId, type, productRefundAmount, productQuantity, productPrice, productRevenue, productSKU, v2ProductName, v2ProductCategory, productVariant, currencyCode, itemQuantity, itemRevenue, transactionRevenue, transactionId, pageTitle, searchKeyword, pagePathLevel1, eCommerceAction_type, eCommerceAction_step, eCommerceAction_option HAVING COUNT(*) > 1 ORDER BY COUNT(*)deSC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAjnUir1holp"
   },
   "source": [
    "### Part 3. Advanced analytics using BigQuery (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2fzmmKfhpkB"
   },
   "outputs": [],
   "source": [
    "## P9. Write a query to list the five products with the most views (product_views) from all visitors (including people who have viewed the same product more than once). Your query counts the number of times a product (v2ProductName) was viewed (product_views), puts the list in descending order, and lists the top 5 entries (5 points).\n",
    "select count(*) as product_views,v2ProductName AS ProductName from `data-to-insights.ecommerce.all_sessions` where type='PAGE' group by v2ProductName  order by product_views desc limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2RHozOShxBS"
   },
   "outputs": [],
   "source": [
    "## P10. Now refine the query to no longer double-count product views for visitors who have viewed a product many times. Each distinct product view should only count once per visitor (10 points).\n",
    "\n",
    "#For finding distinct product viewed by each visitor\n",
    "WITH individual_distinct_product_views AS (\n",
    "SELECT fullVisitorId, v2ProductName \n",
    "FROM `data-to-insights.ecommerce.all_sessions`\n",
    "WHERE type = 'PAGE'\n",
    "GROUP BY fullVisitorId, v2ProductName )\n",
    " \n",
    "#For collecting most viewed products and sort them\n",
    "SELECT\n",
    "  COUNT(*) AS total_product_views, v2ProductName \n",
    "FROM individual_distinct_product_views GROUP BY v2ProductName ORDER BY total_product_views DESC\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGUKfWW6iIoQ"
   },
   "outputs": [],
   "source": [
    "## P11. expand your previous query to include the total number of distinct products ordered and the total number of total units ordered (productQuantity) (5 points).\n",
    "SELECT\n",
    "  COUNT(*) AS total_product_views,\n",
    "  COUNT(Distinct(productQuantity)) AS  total_distinct_products_ordered,\n",
    "  SUM(productQuantity) AS total_ordered_units,\n",
    "  v2ProductName\n",
    "FROM `data-to-insights.ecommerce.all_sessions` WHERE type = 'PAGE' GROUP BY v2ProductName ORDER BY total_product_views DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5oOa66ViSDq"
   },
   "outputs": [],
   "source": [
    "## P12. Expand the query to include the average amount of product per order (total number of units ordered/total number of orders, or SUM(productQuantity)/COUNT(productQuantity)) (5 points).\n",
    "SELECT\n",
    "  COUNT(*) AS total_product_views,\n",
    "  COUNT(productQuantity) AS ordered_units,\n",
    "  SUM(productQuantity) AS Total_ordered_units,\n",
    "  SUM(productQuantity) / COUNT(productQuantity) AS average_amount_of_product_per_order ,\n",
    "  v2ProductName\n",
    "FROM `data-to-insights.ecommerce.all_sessions` WHERE type = 'PAGE' GROUP BY v2ProductName ORDER BY total_product_views DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7IXXWMRiezk"
   },
   "source": [
    "### Part 4. Schedule your analytics queries in ETL pipeline (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvOMtaOGif2a"
   },
   "outputs": [],
   "source": [
    "## P13. Build a pipeline that runs queries you wrote for P4, P5, and P12 (10 points). \n",
    "\n",
    "Explained in the Later cell (Part 4 and 6 Code)\n",
    "\n",
    "## P14. Make sure the queries run in order, P4, then P5, then P15 (5 points) \n",
    "\n",
    "start >> bq_query4 >>bq_query5 >>bq_query12 >> table_export >> end\n",
    "\n",
    "## P14. Schedule the pipeline so that it runs everyday. (5 points)\n",
    "\n",
    "Explained in the Later cell (Part 4 and 6 Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL2Zqe0ojTct"
   },
   "source": [
    "### Part 5. Visualisation in Data Studio (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "261rP0-PjXMm"
   },
   "outputs": [],
   "source": [
    "## P15. Unique visitors by ID: Add the result of P4 as a number to your dashboard (5 points). \n",
    "## P16. Unique visitors by referring site: Add a bar chart that shows the result of the query you wrote in P5 (5 points). \n",
    "## P17. Highly viewed products: Add a pie chart that shows the result of P9 (5 points).\n",
    "## P18. Details of highly viewed products: Add a table that shows the result of P12 in a table (5 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm0LKw4SN96L"
   },
   "source": [
    "** Explained this parts in the video **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LztEDlagjoGb"
   },
   "source": [
    "### Part 6. Exporting the results (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE4LWxVljis3"
   },
   "outputs": [],
   "source": [
    "## P19. Using a command line operation, export the result of P12 to Google Cloud storage (5 points). \n",
    "\n",
    "## The command line code for exporting\n",
    "\n",
    "bq extract --compression GZIP 'dataset_bigdata.datatable' gs://us-central1-assignment3bigd-d3a2ae5a-bucket/output/task6.csv\n",
    "\n",
    "## P20. Add that step as the last step of the Airflow pipeline (5 points).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDIr-9StQ0RJ"
   },
   "source": [
    "## Part 4 and 6 Code:\n",
    "### The Python code of the ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jXaEkPnQ7LY"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import logging\n",
    "from airflow import DAG\n",
    "from airflow import models\n",
    "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators import bash_operator\n",
    "from google.cloud import bigquery #need to import \n",
    "from airflow.operators import bash_operator\n",
    "\n",
    "\n",
    "today_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "# table_name = 'omid.test_results' + '$' + today_date\n",
    "table_name = 'dataset_bigdata.datatable'\n",
    "yesterday = datetime.datetime.combine(\n",
    "datetime.datetime.today() - datetime.timedelta(1),\n",
    "datetime.datetime.min.time())\n",
    "default_dag_args = {\n",
    "# Setting start date as yesterday starts the DAG immediately when it is\n",
    "# detected in the Cloud Storage bucket.\n",
    "'start_date': yesterday,\n",
    "# To email on failure or retry set 'email' arg to your email and enable\n",
    "# emailing here.\n",
    "'email_on_failure': False,\n",
    "'email_on_retry': False,\n",
    "# If a task fails, retry it once after waiting at least 5 minutes\n",
    "'retries': 0,\n",
    "'retry_delay': datetime.timedelta(minutes=5),\n",
    "'project_id': models.Variable.get('gcp_project')\n",
    "}\n",
    "with DAG(dag_id='bigquery-dag-file',\n",
    "\n",
    "\n",
    "# Continue to run DAG once per day\n",
    "               ## P14. Schedule the pipeline so that it runs everyday. (5 points) ##\n",
    "\n",
    "schedule_interval=datetime.timedelta(days=1),\n",
    "default_args=default_dag_args) as dag:\n",
    "\n",
    "\tstart = DummyOperator(task_id='start')\n",
    "\tend = DummyOperator(task_id='end')\n",
    "\tlogging.info('trying to bq_query: ')\n",
    "\tlogging.info('table name: ' + table_name)\n",
    "\n",
    "\tsql1 = f\"\"\" select\n",
    "  COUNT(DISTINCT fullVisitorID) + COUNT(DISTINCT pageviews) as total_unique_visitor\n",
    "from `data-to-insights.ecommerce.all_sessions`  where pageviews is not null\n",
    "\t\"\"\"\n",
    "\tbq_query4 = BigQueryOperator(\n",
    "\ttask_id='big_query4',\n",
    "\tsql=sql1,\n",
    "\tdestination_dataset_table=table_name,\n",
    "\tgcp_conn_id='bigquery_default',\n",
    "\tuse_legacy_sql=False,\n",
    "\twrite_disposition='WRITE_TRUNCATE',\n",
    "\tcreate_disposition='CREATE_IF_NEEDED',\n",
    "\tdag=dag)\n",
    "\n",
    "\tsql2 = f\"\"\" SELECT channelGrouping, COUNT(distinct fullVisitorID) as Unique_Visitors FROM `data-to-insights.ecommerce.all_sessions` group by channelGrouping ORDER BY Unique_Visitors\n",
    "\t\"\"\"\n",
    "\tbq_query5 = BigQueryOperator(\n",
    "\ttask_id='big_query5',\n",
    "\tsql=sql2,\n",
    "\tdestination_dataset_table=table_name,\n",
    "\tgcp_conn_id='bigquery_default',\n",
    "\tuse_legacy_sql=False,\n",
    "\twrite_disposition='WRITE_TRUNCATE',\n",
    "\tcreate_disposition='CREATE_IF_NEEDED',\n",
    "\tdag=dag)\n",
    "\n",
    "\tsql3 = f\"\"\" SELECT\n",
    "  COUNT(*) AS total_product_views,\n",
    "  COUNT(productQuantity) AS ordered_units,\n",
    "  SUM(productQuantity) AS Total_ordered_units,\n",
    "  SUM(productQuantity) / COUNT(productQuantity) AS average_amount_of_product_per_order ,\n",
    "  v2ProductName\n",
    "FROM `data-to-insights.ecommerce.all_sessions` WHERE type = 'PAGE' GROUP BY v2ProductName ORDER BY total_product_views DESC\n",
    "LIMIT 5;\n",
    "\n",
    "\t\"\"\"\n",
    "\tbq_query12 = BigQueryOperator(\n",
    "\ttask_id='big_query12',\n",
    "\tsql=sql3,\n",
    "\tdestination_dataset_table=table_name,\n",
    "\tgcp_conn_id='bigquery_default',\n",
    "\tuse_legacy_sql=False,\n",
    "\twrite_disposition='WRITE_TRUNCATE',\n",
    "\tcreate_disposition='CREATE_IF_NEEDED',\n",
    "\tdag=dag)\n",
    "             ### P20. Add that step as the last step of the Airflow pipeline (5 points).  ###\n",
    "\n",
    "\ttable_export=bash_operator.BashOperator(\n",
    "\t\ttask_id=\"export\",\n",
    "\t\tbash_command=\"bq extract --compression GZIP 'dataset_bigdata.datatable' gs://us-central1-assignment3bigd-d3a2ae5a-bucket/output/task6.csv\" )\n",
    "\n",
    "            ##  P14. Make sure the queries run in order, P4, then P5, then P15 (5 points) ##   \n",
    "start >> bq_query4 >>bq_query5 >>bq_query12 >> table_export >> end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rjlh8eyCLBn9"
   },
   "source": [
    "## A link to the recorded video as instructed in the description \n",
    "### Youtube Link : https://youtu.be/wHF8IzOa8PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA8bFOW1NPMw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
